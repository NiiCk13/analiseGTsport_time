import pandas as pd
import requests
import time
from bs4 import BeautifulSoup
pd.set_option('display.max_rows', 50000)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 500000)

if __name__ == "__main__":
     path_file = input('Inserir o caminho que será salvo o arquivo:')
     print(path_file)
     name_file_user= input('Nome que deseja salvar o arquivo:')
     print(name_file_user)
     nameFile_wExtension =  str(r"\pandas_doc_" + name_file_user + ".csv")
     path_file = str(path_file) + nameFile_wExtension
     site_link = str(input('Inserir o site de extração | Enter para valor default:') or 'https://www.kudosprime.com/gts/rankings.php?sec=daily')
     print('Gerando arquivo...')

     if path_file != '':
            req = requests.get(str(site_link))
            if req.status_code == 200:
                   content = req.content
                   soup = BeautifulSoup(content, 'html5lib')
                   tableTracks = str(soup.table)
                   clearTableTracks = tableTracks.replace('<a', '<p>')
                   df = []
                   df = pd.read_html(clearTableTracks)[0]

                   # The columns Course and Tyres are a unique columns, the both split  above is to give a better lever of granulaty
                   # Get the type of tyres used in the daily race track, str[1] get the value of second column after the split
                   df['Tyres'] = df['Course'].str.split('|').str[1]
                   # Get a name of the daily race track, str[0] get the value of first column after  the split
                   df['Course'] = df['Course'].str.split('|').str[0]

                   df['Leaderboards'] = df['Leaderboards'].str.split('Ends').str[0]
                   df['Leaderboards'] = df['Leaderboards'].str[-25:]
                   df['Leaderboards'] = df['Leaderboards'].str.replace('eid=', '')
                   df['Country'] = df['Leaderboards'].str[-5:]
                   df['Leaderboards'] = df['Leaderboards'].str.replace(',">World', '')
                   df['Leaderboards'] = 'https://www.kudosprime.com/gts/rankings.php?sec=daily&eid=' + df['Leaderboards']
                   df = df.drop(df.loc[0:2].index)

                   # Generate an Excel File
                   df.to_excel('DimensionTable_GtTracks', index=False)
                   df.to_excel(name_file_user +'_Dim_Tracks.xlsx', index=False)

                   def openTables(link):
                          req = requests.get(link)
                          dfTableTime = []

                          if req.status_code == 200:
                                 content = req.content
                                 soup = BeautifulSoup(content, 'html5lib')
                                 tableTimes = str(soup.table)
                                 dfTableTime = pd.read_html(tableTimes)[0]
                                 dfTableTime[['Link']] = link

                          return dfTableTime



                   def appendTables(df, maxInt,path_file):
                          qtdRows = int(maxInt)
                          appended_data = []
                          for i in range(0, qtdRows):
                                 time.sleep(3)
                                 data = pd.DataFrame(openTables(df.loc[i]))
                                 # store DataFrame in list
                                 appended_data.append(data)

                          # see pd.concat documentation for more info
                          appended_data = pd.concat(appended_data)
                          # write DataFrame to an excel sheet
                          appended_data.to_csv('FactTable_gtTimes', index=False)
                          appended_data.to_csv(path_file, index=False)


                   linkTableTimes = appendTables(df['Leaderboards'], len(df['Leaderboards'].index),path_file)
                   print('Arquivo salvo em:'+path_file)



     else:
        path_file = input('Caminho não digitado, favor digitar o caminho:')






